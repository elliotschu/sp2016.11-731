#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict
from _collections import defaultdict


'''
def rawCounts(bitext):
	f_count = defaultdict(int)
	e_count = defaultdict(int)
	fe_count = defaultdict(int)
	#Get raw counts.
	for (n, (f, e)) in enumerate(bitext):
		for f_i in set(f):
			f_count[f_i] += 1
			for e_j in set(e):
				fe_count[(f_i,e_j)] += 1
		for e_j in set(e):
			e_count[e_j] += 1
		if n % 500 == 0:
			sys.stderr.write(".")
	return f_count, e_count, fe_count
'''

def initialize(bitext):
	probs = defaultdict(float)
	f_count = defaultdict(int)

	
	for (n, (f, e)) in enumerate(bitext):
		for f_i in f:
			f_count[f_i] += 1

	#Initialize probabilites for EM
	for (n, (f, e)) in enumerate(bitext):
		for f_i in f:
			for e_i in e:
				probs[f_i,e_i] = 1.0/len(f_count)
		if n % 5000 == 0:
			sys.stderr.write(".")
	return probs

def runEM(bitext, probs):
	EMIterations = 10

	for i in range (EMIterations):
		sys.stderr.write("Beginning EM iteration %i\n" % (i))
		expected_counts = defaultdict(int)
		total_counts = defaultdict(int)
		
		#compute expected counts
		for (n, (f, e)) in enumerate(bitext):
			for e_i in e:
				normTerm = 0
				for f_i in f:
					normTerm += probs[f_i, e_i]
				for f_i in f:
					term = probs[f_i, e_i] / normTerm
					expected_counts[f_i, e_i] += term
					total_counts[f_i] += term
			if n % 5000 == 0:
				sys.stderr.write(".")
		for (n, (f, e)) in enumerate(bitext):
			for f_i in f:
				for e_i in e:
					probs[f_i, e_i] = expected_counts[f_i, e_i] / total_counts[f_i]
			if n % 5000 == 0:
				sys.stderr.write(".")
	return probs

def getAlignments(probs, bitext):
	alignments = []
	for (n, (f, e)) in enumerate(bitext):
		sentAlign = []

		for e_i in e:
			maxScore = 0
			bestPos = 0
			
			for findex in range(0,len(f)):
				if (probs[f[findex], e_i]  > maxScore):
					maxScore = probs[f[findex], e_i]
					bestPos = findex
				
			sentAlign.append(bestPos)
		alignments.append(sentAlign)
	return alignments

def printAlignments(alignments):
 	for a in alignments:
  		for i in range(len(a)):
  			sys.stdout.write("%i-%i " % (a[i], i))
		sys.stdout.write("\n")

def align(opts):
	sys.stderr.write("Training with IBM-1...")
	bitext = [[sentence.strip().split() for sentence in pair.split(' ||| ')] for pair in open(opts.bitext)][:opts.num_sents]
	
	#f_count, _, _ = rawCounts(bitext)
	probs = initialize(bitext)

	#Run EM
	probs = runEM(bitext, probs)

	alignments = getAlignments(probs, bitext)
	
	printAlignments(alignments)



if __name__ == '__main__':
	
	optparser = optparse.OptionParser()
	optparser.add_option("-b", "--bitext", dest="bitext", default="data/dev-test-train.de-en", help="Parallel corpus (default data/dev-test-train.de-en)")
	optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
	optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
	(opts, _) = optparser.parse_args()
	align(opts)


			
			
'''
dice = defaultdict(int)
for (k, (f_i, e_j)) in enumerate(fe_count.keys()):
  dice[(f_i,e_j)] = 2.0 * fe_count[(f_i, e_j)] / (f_count[f_i] + e_count[e_j])
  if k % 5000 == 0:
	sys.stderr.write(".")
sys.stderr.write("\n")

for (f, e) in bitext:
  for (i, f_i) in enumerate(f): 
	for (j, e_j) in enumerate(e):
	  if dice[(f_i,e_j)] >= opts.threshold:
		sys.stdout.write("%i-%i " % (i,j))
  sys.stdout.write("\n")
'''
			